{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e56ApcuBh58L","executionInfo":{"status":"ok","timestamp":1713382136892,"user_tz":-330,"elapsed":22670,"user":{"displayName":"Gurpreet Singh","userId":"09323500649116479200"}},"outputId":"161fe2fc-9354-4cbc-951e-9163d394750e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Install keras==3.1.1 before using model otherwise it will give batch shape error"],"metadata":{"id":"XLyTnnZ1jVZb"}},{"cell_type":"code","source":["!pip install keras==3.1.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":582},"id":"zdm-n5cnjUg7","executionInfo":{"status":"ok","timestamp":1713382512064,"user_tz":-330,"elapsed":12669,"user":{"displayName":"Gurpreet Singh","userId":"09323500649116479200"}},"outputId":"5dccabff-df85-4a30-d106-392dd0da301c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras==3.1.1\n","  Downloading keras-3.1.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras==3.1.1) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras==3.1.1) (1.25.2)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras==3.1.1) (13.7.1)\n","Collecting namex (from keras==3.1.1)\n","  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras==3.1.1) (3.9.0)\n","Collecting optree (from keras==3.1.1)\n","  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras==3.1.1) (0.2.0)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras==3.1.1) (4.11.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras==3.1.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras==3.1.1) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras==3.1.1) (0.1.2)\n","Installing collected packages: namex, optree, keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.15.0\n","    Uninstalling keras-2.15.0:\n","      Successfully uninstalled keras-2.15.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed keras-3.1.1 namex-0.0.8 optree-0.11.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["keras"]},"id":"4256575f22f74dda89d43233db1ed898"}},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KhZ413Gufu0L"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn.preprocessing import LabelEncoder\n","from keras.utils import to_categorical\n","import pandas as pd\n","import numpy as np\n","from collections import Counter\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dropout, Dense,Layer\n","from keras.layers import Attention, Input\n","import keras.backend as K\n","from tensorflow.keras.backend import squeeze,softmax,dot,expand_dims,tanh\n","from tensorflow.keras.backend import sum as sums\n","from keras.models import load_model"]},{"cell_type":"markdown","source":["# For GPU testing"],"metadata":{"id":"BE-XmhIth72W"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","if tf.test.gpu_device_name():\n","    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n","else:\n","    print(\"GPU device not found. Using CPU instead.\")"],"metadata":{"id":"tM9fx0vKh7Ou"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load file for training here Label included in dataset"],"metadata":{"id":"vkGM3-cjhEAG"}},{"cell_type":"code","source":["def load_file(path):\n","    data = pd.read_csv(path)\n","    fields=['src_port', 'dst_port',\n","       'protocol','flow_duration', 'flow_byts_s', 'flow_pkts_s',\n","       'fwd_pkts_s', 'bwd_pkts_s', 'tot_fwd_pkts', 'tot_bwd_pkts',\n","       'totlen_fwd_pkts', 'totlen_bwd_pkts', 'fwd_pkt_len_max',\n","       'fwd_pkt_len_min', 'fwd_pkt_len_mean', 'fwd_pkt_len_std',\n","       'bwd_pkt_len_max', 'bwd_pkt_len_min', 'bwd_pkt_len_mean',\n","       'bwd_pkt_len_std', 'pkt_len_max', 'pkt_len_min', 'pkt_len_mean',\n","       'pkt_len_std', 'pkt_len_var', 'fwd_header_len', 'bwd_header_len', 'flow_iat_mean',\n","       'flow_iat_max', 'flow_iat_min', 'flow_iat_std', 'fwd_iat_tot',\n","       'fwd_iat_max', 'fwd_iat_min', 'fwd_iat_mean', 'fwd_iat_std',\n","       'bwd_iat_tot', 'bwd_iat_max', 'bwd_iat_min', 'bwd_iat_mean',\n","       'bwd_iat_std', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags',\n","       'bwd_urg_flags', 'fin_flag_cnt', 'syn_flag_cnt', 'rst_flag_cnt',\n","       'psh_flag_cnt', 'ack_flag_cnt', 'urg_flag_cnt', 'ece_flag_cnt',\n","       'down_up_ratio', 'pkt_size_avg', 'init_fwd_win_byts',\n","       'init_bwd_win_byts', 'active_max', 'active_min', 'active_mean',\n","       'active_std', 'idle_max', 'idle_min', 'idle_mean', 'idle_std',\n","       'fwd_byts_b_avg', 'fwd_pkts_b_avg', 'bwd_byts_b_avg', 'bwd_pkts_b_avg',\n","       'fwd_blk_rate_avg', 'bwd_blk_rate_avg', 'fwd_seg_size_avg',\n","       'bwd_seg_size_avg', 'cwe_flag_count', 'subflow_fwd_pkts',\n","       'subflow_bwd_pkts', 'subflow_fwd_byts', 'subflow_bwd_byts','Label']\n","    data=data[fields]\n","    return data"],"metadata":{"id":"l1q8CcWXf76E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load file for testing here Label not included in fields"],"metadata":{"id":"deRCE7KrhLhX"}},{"cell_type":"code","source":["def load_file1(path):\n","    data = pd.read_csv(path)\n","    fields=['src_port', 'dst_port',\n","       'protocol','flow_duration', 'flow_byts_s', 'flow_pkts_s',\n","       'fwd_pkts_s', 'bwd_pkts_s', 'tot_fwd_pkts', 'tot_bwd_pkts',\n","       'totlen_fwd_pkts', 'totlen_bwd_pkts', 'fwd_pkt_len_max',\n","       'fwd_pkt_len_min', 'fwd_pkt_len_mean', 'fwd_pkt_len_std',\n","       'bwd_pkt_len_max', 'bwd_pkt_len_min', 'bwd_pkt_len_mean',\n","       'bwd_pkt_len_std', 'pkt_len_max', 'pkt_len_min', 'pkt_len_mean',\n","       'pkt_len_std', 'pkt_len_var', 'fwd_header_len', 'bwd_header_len', 'flow_iat_mean',\n","       'flow_iat_max', 'flow_iat_min', 'flow_iat_std', 'fwd_iat_tot',\n","       'fwd_iat_max', 'fwd_iat_min', 'fwd_iat_mean', 'fwd_iat_std',\n","       'bwd_iat_tot', 'bwd_iat_max', 'bwd_iat_min', 'bwd_iat_mean',\n","       'bwd_iat_std', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags',\n","       'bwd_urg_flags', 'fin_flag_cnt', 'syn_flag_cnt', 'rst_flag_cnt',\n","       'psh_flag_cnt', 'ack_flag_cnt', 'urg_flag_cnt', 'ece_flag_cnt',\n","       'down_up_ratio', 'pkt_size_avg', 'init_fwd_win_byts',\n","       'init_bwd_win_byts', 'active_max', 'active_min', 'active_mean',\n","       'active_std', 'idle_max', 'idle_min', 'idle_mean', 'idle_std',\n","       'fwd_byts_b_avg', 'fwd_pkts_b_avg', 'bwd_byts_b_avg', 'bwd_pkts_b_avg',\n","       'fwd_blk_rate_avg', 'bwd_blk_rate_avg', 'fwd_seg_size_avg',\n","       'bwd_seg_size_avg', 'cwe_flag_count', 'subflow_fwd_pkts',\n","       'subflow_bwd_pkts', 'subflow_fwd_byts', 'subflow_bwd_byts']\n","    data=data[fields]\n","    return data\n","\n","def load_file_old(path):\n","    data = pd.read_csv(path)\n","    fields=[' Source Port',' Destination Port',' Protocol',' Flow Duration','Flow Bytes/s',' Flow Packets/s',\n","        'Fwd Packets/s',' Bwd Packets/s',' Total Fwd Packets',' Total Backward Packets','Total Length of Fwd Packets',' Total Length of Bwd Packets',' Fwd Packet Length Max',\n","       ' Fwd Packet Length Min',' Fwd Packet Length Mean', ' Fwd Packet Length Std','Bwd Packet Length Max',' Bwd Packet Length Min',' Bwd Packet Length Mean',' Bwd Packet Length Std',\n","        ' Max Packet Length',' Min Packet Length',' Packet Length Mean',' Packet Length Std',' Packet Length Variance',' Fwd Header Length',' Bwd Header Length',' Flow IAT Mean',\n","       ' Flow IAT Max',' Flow IAT Min',' Flow IAT Std','Fwd IAT Total',' Fwd IAT Max',' Fwd IAT Min',' Fwd IAT Mean',' Fwd IAT Std','Bwd IAT Total',' Bwd IAT Max',' Bwd IAT Min',\n","       ' Bwd IAT Mean',' Bwd IAT Std','Fwd PSH Flags',' Bwd PSH Flags',' Fwd URG Flags',' Bwd URG Flags','FIN Flag Count',' SYN Flag Count',' RST Flag Count',' PSH Flag Count',\n","        ' ACK Flag Count',' URG Flag Count',' ECE Flag Count',' Down/Up Ratio',' Average Packet Size','Init_Win_bytes_forward',' Init_Win_bytes_backward',' Active Max',' Active Min',\n","        'Active Mean',' Active Std',' Idle Max',' Idle Min','Idle Mean', ' Idle Std','Fwd Avg Bytes/Bulk',' Fwd Avg Packets/Bulk',' Bwd Avg Bytes/Bulk',' Bwd Avg Packets/Bulk',\n","        ' Fwd Avg Bulk Rate','Bwd Avg Bulk Rate',' Avg Fwd Segment Size',' Avg Bwd Segment Size',' CWE Flag Count','Subflow Fwd Packets',' Subflow Bwd Packets',' Subflow Fwd Bytes',\n","        ' Subflow Bwd Bytes']\n","    data=data[fields]\n","    return data"],"metadata":{"id":"qwup2CzQf-f7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = {\n","    '0': \"Benign\",\n","    '1': \"Golden_eye\",\n","    '2': \"vsi\",\n","    '3': \"slow_http\",\n","}"],"metadata":{"id":"pG2X4qXff__h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def format_3d(df):\n","    X = np.array(df)\n","    return np.reshape(X, (X.shape[0], X.shape[1], 1))\n","\n","def preprocess_dataset(dataset):\n","    # Replace 'Infinity' with 0\n","    dataset = dataset.replace('Infinity', '0')\n","    dataset = dataset.replace(np.inf, '0')\n","    # Convert columns to numeric\n","    dataset['flow_pkts_s'] = pd.to_numeric(dataset['flow_pkts_s'])\n","    dataset['flow_byts_s'] = pd.to_numeric(dataset['flow_byts_s'].fillna(0))\n","    print(dataset['Label'].value_counts())\n","    label_encoder = LabelEncoder()\n","    label_encoder.fit(list(labels.values()))\n","    dataset['Label'] = label_encoder.transform(dataset['Label'])\n","    print(dataset['Label'].value_counts())\n","    return dataset\n","\n","def preprocess_dataset1(dataset):\n","    # Replace 'Infinity' with 0\n","    dataset = dataset.replace('Infinity', '0')\n","    dataset = dataset.replace(np.inf, '0')\n","    # Convert columns to numeric\n","    dataset['flow_pkts_s'] = pd.to_numeric(dataset['flow_pkts_s'])\n","    dataset['flow_byts_s'] = pd.to_numeric(dataset['flow_byts_s'].fillna(0))\n","    return dataset\n","\n","def preprocess_dataset_old(samples):\n","  samples = samples.replace('Infinity','0')\n","  samples = samples.replace(np.inf,0)\n","  #samples = samples.replace('nan','0')\n","  samples[' Flow Packets/s'] = pd.to_numeric(samples[' Flow Packets/s'])\n","\n","  samples['Flow Bytes/s'] = samples['Flow Bytes/s'].fillna(0)\n","  samples['Flow Bytes/s'] = pd.to_numeric(samples['Flow Bytes/s'])\n","  return samples\n","\n","def GraphPlot(hist,model):\n","    plt.plot(hist.history['accuracy'])\n","    plt.title('model accuracy')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train'], loc='upper left')\n","    plt.show()\n","    # summarize history for loss\n","    plt.plot(hist.history['loss'])\n","    plt.title('model loss')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train'], loc='upper left')\n","    plt.show()\n","    print(model.metrics_names)\n","\n","def train_test(X,y):\n","    X_train, X_test = X[:-11000], X[-11000:]\n","    y_train, y_test = y[:-11000], y[-11000:]\n","    return  X_train,X_test,y_train,y_test"],"metadata":{"id":"Pwiy_urQgBZ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def save_model(model, name):\n","    # Save the entire model as a .keras file\n","    model.save(name + '.keras')\n","    print('Model Saved')\n","\n","class AttentionLayer(Layer):\n","    def __init__(self, **kwargs):\n","        super(AttentionLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n","        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"zeros\")\n","        super(AttentionLayer, self).build(input_shape)\n","\n","    def call(self, x):\n","        et = squeeze(tanh(dot(x, self.W) + self.b), axis=-1)\n","        at = softmax(et)\n","        at = expand_dims(at, axis=-1)\n","        output = x * at\n","        return sums(output, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], input_shape[-1])\n","\n","def lstmModel(outputlen):\n","    base_model = Sequential()\n","    base_model.add(LSTM(units=50, return_sequences=True, input_shape=( 77,1))) #Changed here\n","    base_model.add(Dropout(0.3))\n","    base_model.add(LSTM(units=50, return_sequences=True))\n","    base_model.add(Dropout(0.3))\n","    base_model.add(AttentionLayer())\n","    base_model.add(Dense(units=4, activation='softmax'))\n","    base_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","    return base_model"],"metadata":{"id":"ebdktIUJgH38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data=load_file('/kaggle/input/traffic1/Labelled_traffic1.csv')\n","\n","fdata=preprocess_dataset(data)\n","x=fdata.drop(columns=['Label']).values\n","x=format_3d(x)\n","y=fdata['Label']\n","y = to_categorical(y, num_classes=4)\n","print(y)"],"metadata":{"id":"QvDH8NgXgC47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# with strategy.scope():\n","X_train,X_test,y_train,y_test=train_test(x,y)\n","y = to_categorical(y, num_classes=4)\n","model=lstmModel(4)\n","history=model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))"],"metadata":{"id":"0Gu9wpMEgOdT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_model(model,'Model')\n","loss, accuracy = model.evaluate(X_test, y_test)\n","# print(f'Loss on test set for {path}: {loss}')\n","# print(f'Accuracy on test set for {path}: {accuracy}')\n","GraphPlot(history,model)"],"metadata":{"id":"Vgfv4LZ5gQO2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# For testing trained model on Dataset"],"metadata":{"id":"UUjY4r7dg6cI"}},{"cell_type":"code","source":["test=load_file1('/content/drive/MyDrive/MachineLearningCSV/DeepDefense-master/liveCapture.csv')"],"metadata":{"id":"9G8LM0qogR5H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = load_model('/content/drive/MyDrive/MachineLearningCSV/DeepDefense-master/03-111/FinalDataset/Notebooks/TrainedModels/ModelvsiLstm.keras', custom_objects={'AttentionLayer': AttentionLayer})"],"metadata":{"id":"39QFwWR6g4xG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processeddata=preprocess_dataset1(test)\n","Reshaped=format_3d(processeddata)\n","prediction=model.predict(processeddata)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QwQ0QgH3gTA-","executionInfo":{"status":"ok","timestamp":1713382902430,"user_tz":-330,"elapsed":1071,"user":{"displayName":"Gurpreet Singh","userId":"09323500649116479200"}},"outputId":"4558cd01-578a-4f71-d4e7-f691893dc4a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step\n"]}]},{"cell_type":"code","source":["predictions = np.argmax(prediction, axis=1)\n","label_counts = Counter(predictions)\n","max_count = 0\n","# print(\"Counts of unique labels:\")\n","for label, count in label_counts.items():\n","    tlabel=labels[str(label)]\n","    print(f\"{tlabel}: {count}\")\n","    if count > max_count:\n","        max_count = count\n","        max_model = tlabel\n","\n","print(f\"Model with maximum predictions: {max_model} ({max_count} predictions)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vKy2skjugUPX","executionInfo":{"status":"ok","timestamp":1713382902430,"user_tz":-330,"elapsed":3,"user":{"displayName":"Gurpreet Singh","userId":"09323500649116479200"}},"outputId":"857652c1-1b45-4520-ae13-ec4be5778640"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Benign: 299\n","Model with maximum predictions: Benign (299 predictions)\n"]}]},{"cell_type":"markdown","source":["# For source ip address of attacker"],"metadata":{"id":"HrlKxRDpgbQ1"}},{"cell_type":"code","source":["\n","fields=['src_port', 'dst_port',\n","       'protocol','flow_duration', 'flow_byts_s', 'flow_pkts_s',\n","       'fwd_pkts_s', 'bwd_pkts_s', 'tot_fwd_pkts', 'tot_bwd_pkts',\n","       'totlen_fwd_pkts', 'totlen_bwd_pkts', 'fwd_pkt_len_max',\n","       'fwd_pkt_len_min', 'fwd_pkt_len_mean', 'fwd_pkt_len_std',\n","       'bwd_pkt_len_max', 'bwd_pkt_len_min', 'bwd_pkt_len_mean',\n","       'bwd_pkt_len_std', 'pkt_len_max', 'pkt_len_min', 'pkt_len_mean',\n","       'pkt_len_std', 'pkt_len_var', 'fwd_header_len', 'bwd_header_len', 'flow_iat_mean',\n","       'flow_iat_max', 'flow_iat_min', 'flow_iat_std', 'fwd_iat_tot',\n","       'fwd_iat_max', 'fwd_iat_min', 'fwd_iat_mean', 'fwd_iat_std',\n","       'bwd_iat_tot', 'bwd_iat_max', 'bwd_iat_min', 'bwd_iat_mean',\n","       'bwd_iat_std', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags',\n","       'bwd_urg_flags', 'fin_flag_cnt', 'syn_flag_cnt', 'rst_flag_cnt',\n","       'psh_flag_cnt', 'ack_flag_cnt', 'urg_flag_cnt', 'ece_flag_cnt',\n","       'down_up_ratio', 'pkt_size_avg', 'init_fwd_win_byts',\n","       'init_bwd_win_byts', 'active_max', 'active_min', 'active_mean',\n","       'active_std', 'idle_max', 'idle_min', 'idle_mean', 'idle_std',\n","       'fwd_byts_b_avg', 'fwd_pkts_b_avg', 'bwd_byts_b_avg', 'bwd_pkts_b_avg',\n","       'fwd_blk_rate_avg', 'bwd_blk_rate_avg', 'fwd_seg_size_avg',\n","       'bwd_seg_size_avg', 'cwe_flag_count', 'subflow_fwd_pkts',\n","       'subflow_bwd_pkts', 'subflow_fwd_byts', 'subflow_bwd_byts']\n","labels = {\n","    '0': \"Benign\",\n","    '1': \"Golden_eye\",\n","    '2': \"vsi\",\n","    '3': \"slow_http\",\n","}\n","\n","def detect_anomaly(csv_path, model):\n","    # Read data\n","    data = pd.read_csv(csv_path)\n","    source_ip = data['src_ip']\n","    data = data[fields]\n","\n","    # Handle special values\n","    dataset = data.replace('Infinity', '0')\n","    dataset = dataset.replace(np.inf, '0')\n","    dataset['flow_pkts_s'] = pd.to_numeric(dataset['flow_pkts_s'])\n","    dataset['flow_byts_s'] = pd.to_numeric(dataset['flow_byts_s'].fillna(0))\n","\n","    # Make predictions\n","    predictions = model.predict(data)\n","    predictions = np.argmax(predictions, axis=1)\n","\n","    # Count predictions\n","    label_counts = Counter(predictions)\n","    max_count = 0\n","    max_model=''\n","    for label, count in label_counts.items():\n","        tlabel = labels[str(label)]\n","        print(f\"{tlabel}: {count}\")\n","        if count > max_count and tlabel != \"Benign\":\n","            max_count = count\n","            max_model = tlabel\n","\n","    print(f\"Model with maximum predictions: {max_model} ({max_count} predictions)\")\n","\n","    # Store anomalies\n","    unique_ips = set()  # To store unique IPs\n","    anomalies = []\n","    for i in range(len(predictions)):\n","        if predictions[i] != \"Benign\" and labels[str(predictions[i])] == max_model:\n","            ip = source_ip[i]\n","            if ip not in unique_ips:  # Check if IP is already seen\n","                unique_ips.add(ip)\n","                anomalies.append((ip, labels[str(predictions[i])]))\n","\n","    # Print the results\n","    print('Source IP | Anomaly')\n","    print('===================')\n","    for ip, anomaly in anomalies:\n","        print(f\"{ip} | {anomaly}\")"],"metadata":{"id":"CLrDtjhbgVRg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["detect_anomaly('/content/drive/MyDrive/MachineLearningCSV/DeepDefense-master/liveCapture.csv', model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"sHVWngRlgXMs","executionInfo":{"status":"error","timestamp":1713382958329,"user_tz":-330,"elapsed":624,"user":{"displayName":"Gurpreet Singh","userId":"09323500649116479200"}},"outputId":"5137bc2f-0c36-4e3a-9aaa-17b03c806c6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","Benign: 299\n"]},{"output_type":"error","ename":"UnboundLocalError","evalue":"local variable 'max_model' referenced before assignment","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-55e705d72c0c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdetect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/MachineLearningCSV/DeepDefense-master/liveCapture.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-33-e2da21828fba>\u001b[0m in \u001b[0;36mdetect_anomaly\u001b[0;34m(csv_path, model)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mmax_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model with maximum predictions: {max_model} ({max_count} predictions)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Store anomalies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'max_model' referenced before assignment"]}]}]}